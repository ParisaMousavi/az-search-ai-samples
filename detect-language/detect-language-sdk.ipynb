{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c172719-88bb-4ee2-a942-8b9bdf6b5a94",
   "metadata": {},
   "source": [
    "# Detect language with SDK\n",
    "\n",
    "Reference document\n",
    "https://microsoftlearning.github.io/AI-102-AIEngineer/Instructions/01-get-started-cognitive-services.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cfbd613-f072-439a-a14d-1267bab6b560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-ai-textanalytics==5.3.0\n",
      "  Downloading azure_ai_textanalytics-5.3.0-py3-none-any.whl.metadata (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting azure-core<2.0.0,>=1.24.0 (from azure-ai-textanalytics==5.3.0)\n",
      "  Downloading azure_core-1.29.6-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting azure-common~=1.1 (from azure-ai-textanalytics==5.3.0)\n",
      "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Collecting isodate<1.0.0,>=0.6.1 (from azure-ai-textanalytics==5.3.0)\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from azure-ai-textanalytics==5.3.0) (4.8.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/codespace/.local/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.3.0) (4.1.0)\n",
      "Requirement already satisfied: requests>=2.21.0 in /home/codespace/.local/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.3.0) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /home/codespace/.local/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.3.0) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5.0,>=3.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.3.0) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5.0,>=3.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.3.0) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5.0,>=3.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.3.0) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.3.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.3.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics==5.3.0) (2023.11.17)\n",
      "Downloading azure_ai_textanalytics-5.3.0-py3-none-any.whl (298 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.6/298.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading azure_core-1.29.6-py3-none-any.whl (192 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.5/192.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: azure-common, isodate, azure-core, azure-ai-textanalytics\n",
      "Successfully installed azure-ai-textanalytics-5.3.0 azure-common-1.1.28 azure-core-1.29.6 isodate-0.6.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    " !pip install azure-ai-textanalytics==5.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38873135-fc21-4b34-b12c-1d37eecd6399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import http.client, base64, json, urllib\n",
    "from urllib import request, parse, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2cff7d6-08df-4bb4-902b-5a0b030d7d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter some text (\"quit\" to stop)\n",
      " La connaissance est meilleure.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"documents\": [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"La connaissance est meilleure.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"documents\": [\n",
      "    {\n",
      "      \"id\": \"1\",\n",
      "      \"detectedLanguage\": {\n",
      "        \"name\": \"French\",\n",
      "        \"iso6391Name\": \"fr\",\n",
      "        \"confidenceScore\": 1.0\n",
      "      },\n",
      "      \"warnings\": []\n",
      "    }\n",
      "  ],\n",
      "  \"errors\": [],\n",
      "  \"modelVersion\": \"2022-10-01\"\n",
      "}\n",
      "\n",
      "Language: French\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter some text (\"quit\" to stop)\n",
      " Knowledge is better.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"documents\": [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"Knowledge is better.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"documents\": [\n",
      "    {\n",
      "      \"id\": \"1\",\n",
      "      \"detectedLanguage\": {\n",
      "        \"name\": \"English\",\n",
      "        \"iso6391Name\": \"en\",\n",
      "        \"confidenceScore\": 0.98\n",
      "      },\n",
      "      \"warnings\": []\n",
      "    }\n",
      "  ],\n",
      "  \"errors\": [],\n",
      "  \"modelVersion\": \"2022-10-01\"\n",
      "}\n",
      "\n",
      "Language: English\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter some text (\"quit\" to stop)\n",
      " quit\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    global cog_endpoint\n",
    "    global cog_key\n",
    "    try:\n",
    "        # Get Configuration Settings\n",
    "        load_dotenv()\n",
    "        cog_endpoint = os.getenv('COG_SERVICE_ENDPOINT')\n",
    "        cog_key = os.getenv('COG_SERVICE_KEY')\n",
    "        \n",
    "        # Get user input (until they enter \"quit\")\n",
    "        userText =''\n",
    "        while userText.lower() != 'quit':\n",
    "            userText = input('Enter some text (\"quit\" to stop)\\n')\n",
    "            if userText.lower() != 'quit':\n",
    "                GetLanguage(userText)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "def GetLanguage(text):\n",
    "    try:\n",
    "        # Construct the JSON request body (a collection of documents, each with an ID and text)\n",
    "        jsonBody = {\n",
    "            \"documents\":[\n",
    "                {\"id\": 1,\n",
    "                 \"text\": text}\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Let's take a look at the JSON we'll send to the service\n",
    "        print(json.dumps(jsonBody, indent=2))\n",
    "\n",
    "        # Make an HTTP request to the REST interface\n",
    "        uri = cog_endpoint.rstrip('/').replace('https://', '')\n",
    "        conn = http.client.HTTPSConnection(uri)\n",
    "\n",
    "        # Add the authentication key to the request header\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Ocp-Apim-Subscription-Key': cog_key\n",
    "        }\n",
    "\n",
    "        # Use the Text Analytics language API\n",
    "        conn.request(\"POST\", \"/text/analytics/v3.1/languages?\", str(jsonBody).encode('utf-8'), headers)\n",
    "\n",
    "        # Send the request\n",
    "        response = conn.getresponse()\n",
    "        data = response.read().decode(\"UTF-8\")\n",
    "\n",
    "        # If the call was successful, get the response\n",
    "        if response.status == 200:\n",
    "\n",
    "            # Display the JSON response in full (just so we can see it)\n",
    "            results = json.loads(data)\n",
    "            print(json.dumps(results, indent=2))\n",
    "\n",
    "            # Extract the detected language name for each document\n",
    "            for document in results[\"documents\"]:\n",
    "                print(\"\\nLanguage:\", document[\"detectedLanguage\"][\"name\"])\n",
    "\n",
    "        else:\n",
    "            # Something went wrong, write the whole response\n",
    "            print(data)\n",
    "\n",
    "        conn.close()\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c1a02f-7d54-4c8e-97ae-d482848adc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
